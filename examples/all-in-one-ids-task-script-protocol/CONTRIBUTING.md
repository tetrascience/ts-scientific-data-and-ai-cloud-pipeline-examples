# Developing demo-ssp <!-- omit in toc -->

# Table of Contents <!-- omit in toc -->

- [Getting started](#getting-started)
- [Installation](#installation)
- [Running tests and updating auto-generated files](#running-tests-and-updating-auto-generated-files)
- [Upload artifacts to TDP](#upload-artifacts-to-tdp)
  - [Upload IDS](#upload-ids)
  - [Upload task script](#upload-task-script)
    - [Preparing requirements.txt for Python dependencies](#preparing-requirementstxt-for-python-dependencies)
    - [Upload the task script artifact](#upload-the-task-script-artifact)
  - [Upload protocol](#upload-protocol)
- [Create a pipeline](#create-a-pipeline)
- [Artifact identity (namespace, type slug and version)](#artifact-identity-namespace-type-slug-and-version)

## Getting started

This example folder structure contains an IDS, task script and protocol for use with self-service pipelines.
These three artifacts can be uploaded to the Tetra Data Platform (TDP) and used to create a data pipeline.

More information about self-service artifacts can be found in the documentation here: https://developers.tetrascience.com/docs/self-service-pipelines-overview

This project includes:

- A complete example of an IDS, task script and protocol all in one folder structure, which can be uploaded and used as a self-service pipeline.
- A programmatic definition of an IDS using `ts-ids-core`.
- Examples of testing with `pytest`, including mocked responses from the `context` interface from `ts-sdk`.
- Use of `poetry` for Python dependency management.

The IDS artifact is contained in the `ids` folder.
The protocol artifact is contained in the `protocol` folder.
The task script artifact is the entire root folder of the project (_not_ just the `task_script` folder which contains Python modules for the task script).

The key folders and files are listed below:

```
.
├── CONTRIBUTING.md - Developer instructions for working in this project
├── ids - IDS artifact files
│  ├── __init__.py
│  ├── athena.json - Athena specification
│  ├── elasticsearch.json - Elasticsearch specification
│  ├── expected.json - Example IDS JSON data (generated by a snapshot test)
│  ├── expected.py - Example programmatic IDS data
│  ├── manifest.json - Metadata about the IDS
│  ├── README.md - Documentation viewable in TDP for this IDS
│  ├── schema.json - JSON Schema
│  └── schema.py - Programmatic IDS - add Python modules and sub-packages as needed for larger schemas
├── protocol - Protocol artifact files
│  ├── manifest.json - Metadata about the protocol
│  ├── protocol.yml - The protocol definition
│  └── README.md - Documentation viewable in TDP for this protocol
├── config.json - The task script configuration file
├── main.py - The entrypoint to the task script (as specified in `config.json`)
├── manifest.json - Metadata about the task script
├── README.md - Documentation viewable in TDP for this task script
├── example-input - Example input files which would be uploaded to the data lake
├── example-output - Example output files generated from the input files
├── __tests__ - `pytest` tests including integration and unit tests
├── pyproject.toml - The `poetry` project definition
└── task_script - Python sub-package containing task script logic called from `main.py`
   ├── __init__.py
   └── parse.py - Task script logic. Add more Python modules and sub-packages as needed for larger task scripts
```

For larger projects where multiple IDSs, task scripts or protocols need to be created together, it can be beneficial to create separate repositories for the IDS(s), task script(s) and protocol(s) instead of keeping them together in one package.
This package structure is intended to be an all-in-one example of a straightforward IDS, task script and protocol.

After copying this example, initialize it as a git repository to keep track of changes.

## Installation

This project is using `poetry` to manage Python dependencies.

For initial setup, update `pyproject.toml` with a package source which makes `ts-ids-core` available in order to create an IDS programmatically.

A typical developer setup, including selecting the python interpreter with `pyenv`, is:

```sh
pyenv local 3.11
poetry env use 3.11
poetry install
```

## Running tests and updating auto-generated files

Tests are located in the `__tests__` directory.

To run all tests, run `poetry run pytest`.

The tests include snapshot tests, which compare the output of a test with the content of a file, and can be used to automatically update those snapshot files with the expected content.
Snapshot tests are used to update the following files:

- `schema.json` is compared with the JSON schema produced by the definition in `schema.py`
- `expected.json` is compared with the output from `expected.py`
- `elasticsearch.json` is compared with the output `ts-ids-es-json-generator`, which produces a simple default Elasticsearch mapping from the schema.
  - This test can be deleted if you would prefer to create a custom Elasticsearch mapping. Note that the Elasticsearch mapping is also checked as part of `ts-ids-validator` so any mistakes leading to an invalid mapping will be caught before the IDS is uploaded to TDP.
- All files in `example-output` are generated by taking files from `example-input` as inputs to the main task script function, which produces IDS JSON files which are saved to `example-output`.

These snapshot files don't need to be maintained manually - after making changes in the repo, update snapshot files by running `poetry run pytest --snapshot-update`.
The modified snapshot files can be checked with `git diff` before being committed.

## Upload artifacts to TDP

Get started with self-service pipelines by following documentation here: <https://developers.tetrascience.com/docs/self-service-pipelines-overview>.
Before uploading artifacts you will need to choose a unique combination of namespace, type slug and version - see the section on artifact identity below.

After getting set up, the `ts-sdk` command line script is available for uploading artifacts.

It is possible to store TDP credentials as environment variables so that they don't have to be passed in to the command with `--config` each time.
The example below sets the environment variables in a unix shell:

```sh
export TS_ORG=<your-org-slug>
export TS_API_URL=https://api.tetrascience.com/v1
export TS_AUTH_TOKEN=<token>
```

The approach depends on the specific terminal being used.
These environment variables can be set globally, for example by putting them into `~/.bashrc` when using `bash`.

After installing `ts-sdk` according to the documentation above, this IDS can be uploaded to TDP like this:

```sh
ts-sdk put ids <namespace> <slug> <version> <artifact-folder>
```

### Upload IDS

```sh
ts-sdk put ids private-example demo-ssp v0.1.0 ./ids
```

### Upload task script

#### Preparing requirements.txt for Python dependencies

Before uploading the task script, we need to create a `requirements.txt` file which is used by the TDP build process to install Python dependencies.
This demo uses privately hosted Python packages which are not available publicly on PyPI: `ts-ids-core` and `ts-ids-components`.
The task script build process only has access to publicly hosted packages from PyPI, so to make these dependencies available we will first download them to a `dependencies/` folder using a script.

The script to generate `requirements.txt` is made available as a command-line script `generate-requirements` which can be run in the virtual environment created by `poetry` after `poetry install`.
The script runs `poetry` and `pip` as subprocesses: to inspect what the script does before running it, check `task_script/tools/prepare_requirements.py`.

After the script has run without error, `requirements.txt` and `dependencies/` should be populated.
Both of these are temporary and can always be regenerated by running the script again, so it is safe to delete them and they are excluded from version control in `.gitignore`.

#### Upload the task script artifact

```sh
generate-requirements
ts-sdk put task-script private-example demo-ssp v0.1.0 .
```

### Upload protocol

```sh
ts-sdk put protocol private-example demo-ssp v0.1.0 ./protocol
```

## Create a pipeline

After uploading all artifacts (IDS, task script and protocol), you can go to the Tetra Data Platform and set up a new pipeline by selecting the protocol by its name.
The full documentation for creating pipelines is here: https://developers.tetrascience.com/docs/managing-pipelines

## Artifact identity (namespace, type slug and version)

The namespace of this example is `private-example`, the type slug is `demo-ssp`, and the version is `v0.1.0`.
This is often put together in one string as `private-example/demo-ssp:v0.1.0`.

The combination of the kind of artifact (IDS, task script or protocol), namespace, slug and version uniquely identifies an artifact.
If the IDS `private-example/demo-ssp:v0.1.0` already exists, there will be an error message when trying to upload another IDS with the same identity using `ts-sdk put`.
When first setting up a new project, choose a new unique slug by updating the files explained below.
After making changes to an existing artifact, bump the version number before uploading.

In this example, the slug `demo-ssp` is the same for the IDS, task script and protocol, but it is also possible for each to have a different slug.

The IDS namespace, type slug and version are defined in these files:

```
.
├── ids
│  ├── manifest.json
│  └── schema.py - `Model.NAMESPACE`, `Model.TYPE` and `Model.VERSION`
└── config.json - The task script specifies its IDS output type in "allowedIds"
```

Additionally, by running `poetry run pytest --snapshot-update` after changing the above files, the namespace, slug and version will be updated in `ids/schema.json`, `ids/expected.json`, and the IDS JSON files in `example-output`.

The task script namespace, type slug and version are defined in these files:

```
.
├── manifest.json
└── protocol
   └── protocol.yml - The protocol specifies which task script(s) it uses in `steps`
```

The protocol namespace, type slug and version are defined in these files:

```
.
└── protocol
   └── manifest.json
```
