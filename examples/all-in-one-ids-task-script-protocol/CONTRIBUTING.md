# Developing demo-ssp <!-- omit in toc -->

## Table of Contents <!-- omit in toc -->

- [Quickstart](#quickstart)
  - [Speed run](#speed-run)
- [Overview](#overview)
- [Installation](#installation)
- [Running tests and updating auto-generated files](#running-tests-and-updating-auto-generated-files)
- [Upload artifacts to TDP](#upload-artifacts-to-tdp)
  - [About preparing requirements.txt for Python dependencies](#about-preparing-requirementstxt-for-python-dependencies)
- [Create a pipeline](#create-a-pipeline)
- [Artifact identity (namespace, type slug and version)](#artifact-identity-namespace-type-slug-and-version)

## Quickstart

Before getting started, replace the following placeholders throughout the files:

- `{YOUR_ORG_SLUG}` - your organization slug
- `{YOUR_SLUG_PREFIX}` - The artifacts use the slug `{YOUR_SLUG_PREFIX}demo-ssp`. You can replace the placeholders to add a prefix to the slugs. For example, you can set this to your username followed by a hyphen, e.g., `jfoldager-`

Follow the instructions in [Speed run](#speed-run) or see the sections [Installation](#installation) and [Upload artifacts to TDP](#upload-artifacts-to-tdp) for more detailed next steps.

### Speed run

> [!NOTE]
> The commands assume that you are in the folder `examples/all-in-one-ids-task-script-protocol`.
>
> Install the `ts-cli` command line tool if you don't have it already:
>
> ```sh
> pipx install tetrascience-cli
> # or if you don't have pipx:
> pip install tetrascience-cli
> ```

Install the root package, which includes the helper tool `generate-requirements`

```sh
poetry install

# Install the task script. This is needed for generate-requirements to work
poetry install --project task_script
```

Generate requirements.txt and download private dependencies for the task script build process. These are stored in the `task_script/dependencies` folder.

```sh
poetry run generate-requirements
```

Deploy the IDS, task script and protocol

```sh
ts-cli publish ids/ --config {path to ts-sdk-cfg.json}
ts-cli publish task_script/ --config {path to ts-sdk-cfg.json}
ts-cli publish protocol/ --config {path to ts-sdk-cfg.json}
```

## Overview

This example folder structure contains an IDS, task script and protocol for use with self-service pipelines.
These three artifacts can be uploaded to the Tetra Data Platform (TDP) and used to create a data pipeline.

More information about self-service artifacts can be found in the documentation here: https://developers.tetrascience.com/docs/self-service-pipelines-overview

This project includes:

- A complete example of an IDS, task script and protocol all in one folder structure, which can be uploaded and used as a self-service pipeline.
- A programmatic definition of an IDS using `ts-ids-core`.
- Examples of testing with `pytest`, including mocked responses from the `context` interface from `ts-sdk`.
- Use of `poetry` for Python dependency management.

The IDS artifact is contained in the `ids` folder.
The protocol artifact is contained in the `protocol` folder.
The task script artifact is contained in the `task_script` folder.

The key folders and files are listed below:

```
.
├── CONTRIBUTING.md - Developer instructions for working in this project
├── ids - IDS artifact files
│  ├── __init__.py
│  ├── __tests__ - Tests for the IDS schema
│  ├── athena.json - Athena specification
│  ├── demo_ssp_ids - IDS package containing programmatic schema
│  │  ├── __init__.py
│  │  ├── expected.py - Example programmatic IDS data
│  │  └── schema.py - Programmatic IDS - add Python modules and sub-packages as needed for larger schemas
│  ├── elasticsearch.json - Elasticsearch specification
│  ├── expected.json - Example IDS JSON data (generated by a snapshot test)
│  ├── manifest.json - Metadata about the IDS
│  ├── README.md - Documentation viewable in TDP for this IDS
│  ├── schema.json - JSON Schema (generated from schema.py)
│  ├── pyproject.toml - Poetry project definition for IDS
│  └── poetry.lock - Locked dependencies for IDS
├── protocol - Protocol artifact files
│  ├── manifest.json - Metadata about the protocol
│  ├── protocol.yml - The protocol definition
│  └── README.md - Documentation viewable in TDP for this protocol
├── task_script - Task script artifact files
│  ├── __init__.py
│  ├── __tests__ - `pytest` tests including integration and unit tests
│  ├── config.json - The task script configuration file
│  ├── main.py - The entrypoint to the task script (as specified in `config.json`)
│  ├── manifest.json - Metadata about the task script
│  ├── README.md - Documentation viewable in TDP for this task script
│  ├── demo_ssp_task_script - Task script package containing logic
│  │  └── parse.py - Task script logic. Add more Python modules and sub-packages as needed for larger task scripts
│  ├── tools - Utility scripts for task script development
│  │  └── generate_requirements.py - Script to generate requirements.txt
│  ├── pyproject.toml - Poetry project definition for task script
│  ├── poetry.lock - Locked dependencies for task script
│  └── requirements.txt - Generated Python dependencies for TDP deployment
├── example-input - Example input files which would be uploaded to the data lake
├── example-output - Example output files generated from the input files
└── README.md - Documentation
```

For larger projects where multiple IDSs, task scripts or protocols need to be created together, it can be beneficial to create separate repositories for the IDS(s), task script(s) and protocol(s) instead of keeping them together in one package.
This package structure is intended to be an all-in-one example of a straightforward IDS, task script and protocol.

After copying this example, initialize it as a git repository to keep track of changes.

## Installation

This project is using `poetry` to manage Python dependencies.

> [!IMPORTANT]
> If you have an active virtual environment, you will need to deactivate it before running `poetry install` by running `deactivate`.

```sh

# Install helper tool for generating requirements.txt and bundling private dependencies for the task script build process
poetry install

# Install IDS in the ids folder
poetry install --project ids

# Install task script in the task_script folder
poetry install --project task_script

```

## Running tests and updating auto-generated files

Tests are located in the `ids/__tests__` and `task_script/__tests__` directories.

To run all tests, run `poetry run pytest` in both folders.

The tests include snapshot tests, which compare the output of a test with the content of a file, and can be used to automatically update those snapshot files with the expected content.
Snapshot tests are used to update the following files:

- `ids/schema.json` is compared with the JSON schema produced by the definition in `ids/demo_ssp_ids/schema.py`
- `ids/expected.json` is compared with the output from `ids/demo_ssp_ids/expected.py`
- `ids/elasticsearch.json` is compared with the output `ts-ids-es-json-generator`, which produces a simple default Elasticsearch mapping from the schema.
  - This test can be deleted if you would prefer to create a custom Elasticsearch mapping. Note that the Elasticsearch mapping is also checked as part of `ts-ids-validator` so any mistakes leading to an invalid mapping will be caught before the IDS is uploaded to TDP.
- All files in `example-output` are generated by taking files from `example-input` as inputs to the main task script function in `task_script/main.py`, which produces IDS JSON files which are saved to `example-output`.

These snapshot files don't need to be maintained manually - after making changes in the repo, update snapshot files by running `poetry run pytest --snapshot-update`.
The modified snapshot files can be checked with `git diff` before being committed.

## Upload artifacts to TDP

Get started with self-service pipelines by following documentation here: <https://developers.tetrascience.com/docs/self-service-pipelines-overview>.
Before uploading artifacts you will need to choose a unique combination of namespace, type slug and version - see the section on artifact identity below.

After getting set up, the `ts-cli` command line script is available for uploading artifacts and can be used to upload all three artifacts.

The ids, task script and protocol depend on each other, and should be uploaded in the following order:

1. IDS
2. Task script
3. Protocol

The task script has a build step which needs to be run before uploading the task script.
The build step packages some of the dependencies into a `dependencies/` folder which is then included when uploading the task script. This step is needed when relying on private Python packages which are not available publicly on PyPI.

The artifacts are built and uploaded using the following commands:

```sh
ts-cli publish ids/ --config {path to ts-sdk-cfg.json}
```

```sh
poetry run generate-requirements
ts-cli publish task_script/ --config {path to ts-sdk-cfg.json}
```

```sh
ts-cli publish protocol/ --config {path to ts-sdk-cfg.json}
```

### About preparing requirements.txt for Python dependencies

Before uploading the task script, we need to create a `requirements.txt` file which is used by the TDP build process to install Python dependencies.
This demo uses privately hosted Python packages which are not available publicly on PyPI: `ts-ids-core` and `ts-ids-components`.
The task script build process only has access to publicly hosted packages from PyPI, so to make these dependencies available we will first download them to a `dependencies/` folder using a script.

The script to generate `requirements.txt` is made available as a command-line script `generate-requirements` which can be run in the virtual environment created by `poetry` after `poetry install`.
The script runs `poetry` and `pip` as subprocesses: to inspect what the script does before running it, check `task_script/tools/generate_requirements.py`.

After the script has run without error, `task_script/requirements.txt` and `task_script/dependencies/` should be populated.
Both of these are temporary and can always be regenerated by running the script again, so it is safe to delete them and they are excluded from version control in `.gitignore`.

## Create a pipeline

After uploading all artifacts (IDS, task script and protocol), you can go to the Tetra Data Platform and set up a new pipeline by selecting the protocol by its name.
The full documentation for creating pipelines is here: https://developers.tetrascience.com/docs/managing-pipelines

## Artifact identity (namespace, type slug and version)

The namespace of this example is `private-training-onboarding`, the type slug is `demo-ssp`, and the version is `v0.1.0`.
This is often put together in one string as `private-training-onboarding/demo-ssp:v0.1.0`.

The combination of the kind of artifact (IDS, task script or protocol), namespace, slug and version uniquely identifies an artifact.

In this example, the slug `demo-ssp` is the same for the IDS, task script and protocol, but it is also possible for each to have a different slug.

The IDS namespace, type slug and version are defined in these files:

```
.
├── ids
│  ├── manifest.json
│  └── demo_ssp_ids
│     └── schema.py - `Model.NAMESPACE`, `Model.TYPE` and `Model.VERSION`
└── task_script
   └── config.json - The task script specifies its IDS output type in "allowedIds"
```

Additionally, by running `poetry run pytest --snapshot-update` after changing the above files, the namespace, slug and version will be updated in `ids/schema.json`, `ids/expected.json`, and the IDS JSON files in `example-output`.

The task script namespace, type slug and version are defined in these files:

```
.
├── task_script
│  └── manifest.json
└── protocol
   └── protocol.yml - The protocol specifies which task script(s) it uses in `steps`
```

The protocol namespace, type slug and version are defined in these files:

```
.
└── protocol
   └── manifest.json
```
